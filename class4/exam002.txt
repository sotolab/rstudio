# KoNLP 라이브러리 

# 1 KoNLP 패키지 로딩
> library(KoNLP)
> library(tm)
> library(wordcloud)
> user_dic <- data.frame( term = c("R프로그래밍", "페이스북","스타", "썬"), tag = 'ncn')
> buildDictionary(ext_dic = 'sejong', user_dic = user_dic)

1: All
2: CRAN packages only
3: None

[Enter]

Are you sure?

1: Yes
2: No

선택: 1

** testing if installed package keeps a record of temporary installation path
* DONE (NIADic)
370961 words dictionary was built.

>

# 2 단어 추출
# readLines : 줄 단위로 데이터 생성
# head : 앞부분 6줄 보기
# extractNoun : 명사추출
# collapse = "" : 공백제거
# sapply (자료집, 사용자정의함수) : 사용자 정의 함수를 이용하여 단어집에서 단어를 추출

> setwd("C:/workspace/rstudio-master/data")
> face_book <- file("facebook_bigdata.txt", encoding = "UTF-8")
> facebook_data <- readLines(face_book)
> head(facebook_data)
[1] "﻿﻿스마트 기기와 SNS 덕분에 과거 어느 때보다 많은 데이터가 흘러 다니고 빠르게 쌓입니다. 
다음 그림은 2013년에 인터넷에서 60초 동안 얼마나 많은 일이 벌어지는지를 나타낸 그림이다. 
Facebook에서는 1초마다 글이 4만 천 건 포스팅되고, 좋아요 클릭이 180만 건 발생합니다. 
데이터는 350GB씩 쌓입니다. 
이런 데이터를 실시간으로 분석하면 사용자의 패턴을 파악하거나 의사를 결정하는 데 참고하는 등
다양하게 사용할 수 있을 것입니다. "    

> exNouns <- function(x) { paste(extractNoun(as.character(x)), collapse = "") }
> facebook_nouns <- sapply(facebook_data, exNouns)
> facebook_nouns[1]
﻿﻿스마트 기기와 SNS 덕분에 과거 어느 때보다 많은 데이터가 흘러 다니고 빠르게 쌓입니다. 다음 그림은 2013년에 인터넷에서 60초 동안 얼마나 많은 일이 벌어지는지를 나타낸 그림이다. Facebook에서는 1초마다 글이 4만 천 건 포스팅되고, 좋아요 클릭이 180만 건 발생합니다. 데이터는 350GB씩 쌓입니다. 이런 데이터를 실시간으로 분석하면 사용자의 패턴을 파악하거나 의사를 결정하는 데 참고하는 등 다양하게 사용할 수 있을 것입니다.  
                                                                                                                                                                                                                                           "﻿﻿스마트기SNS덕분과거때데이터다음그림2013년인터넷60초동안일지그림Facebook1초글4천거포스팅되고클릭180거발생데이터350GB씩데이터실시간분석사용자패턴파악의사결정데등다양하게사용수것" 

# 3 추출된 단어를 대상으로 전처리
# 추출된 단어를 이용하여 말뭉치(corpus) 생성
# 문자부호 제거: removePunctuation
# 수치 제거 : removeNumbers
# 소문자 변경 : tolower
# 불용어 제거 : removeWords, stopwords('english')
# 전처리 결과 확인 : inspect
> myCorpus <- Corpus(VectorSource(facebook_nouns))
> myCorpus
<<SimpleCorpus>>
Metadata:  corpus specific: 1, document level (indexed): 0
Content:  documents: 76
> 
> myCorpusPrepro <- tm_map(myCorpus, removePunctuation)
> myCorpusPrepro <- tm_map(myCorpusPrepro, removeNumbers)
> myCorpusPrepro <- tm_map(myCorpusPrepro, tolower)
> myCorpusPrepro <- tm_map(myCorpusPrepro, removeWords, stopwords('english'))
> inspect(myCorpusPrepro[1:5])
<<SimpleCorpus>>
Metadata:  corpus specific: 1, document level (indexed): 0
Content:  documents: 5

# 4 단어 선별
# TermDocumentMatrix : 분석에 필요한 단어를 선별하고, 단어/문서 행렬을 생성
> myCorpusPrepro_term <- TermDocumentMatrix(myCorpusPrepro,
+ control = list(wordLengths = c(4, 16)))
> myCorpusPrepro_term
<<TermDocumentMatrix (terms: 692, documents: 76)>>
Non-/sparse entries: 1255/51337
Sparsity           : 98%
Maximal term length: 12
Weighting          : term frequency (tf)

> myTerm_df <- as.data.frame(as.matrix(myCorpusPrepro_term))
> dim(myTerm_df)
[1] 692  76

# 5 단어 출현 빈도수 
# 출현 빈도수가 높은 순서대로 내림차순 정렬
# rowSums : 동일한 단어의 빈도수를 계산
# sort : 빈도수를 descreasing = TRUE 속성
# 가장 높은 상위 10개를 추출
> wordResult <- sort(rowSums(myTerm_df), decreasing = TRUE)
> wordResult[1:10]
  데이터     분석 빅데이터     처리     사용     수집   시스템     저장     결과     노드 
      91       41       33       31       29       27       23       16       14       13 
> 
# 6 어울리지 않는 단어 제거 
# 
> myCorpusPrepro <- tm_map(myCorpus, removePunctuation)
> myCorpusPrepro <- tm_map(myCorpusPrepro, removeNumbers)
> myCorpusPrepro <- tm_map(myCorpusPrepro, tolower)
> myStopwords = c(stopwords('english'), "사용", "하기")
> myCorpusPrepro <- tm_map(myCorpusPrepro, removeWords, myStopwords)

> myCorpusPrepro_term <- TermDocumentMatrix(myCorpusPrepro,
+ control = list(wordLengths = c(4, 16)))
> myTerm_df <- as.data.frame(as.matrix(myCorpusPrepro_term))
> wordResult <- sort(rowSums(myTerm_df), decreasing = TRUE)
> wordResult[1:10]
  데이터     분석 빅데이터     처리     수집   시스템     저장     결과     노드     얘기 
      91       41       33       31       27       23       16       14       13       13

# 7 워드클라우스 생성
# scale = c(5,1) : 비율크기
# min.freq = 3 : 최소 빈도수
# random.order = F : 램덤 순서
# rot.per = .1 : 회전비율
# colors = pal : 색상(파렛트)
# family = "malgun" : 글꼴
> myName <- names(wordResult)
> word.df <- data.frame(word = myName, freq = wordResult)
> str(word.df)
'data.frame':	690 obs. of  2 variables:
 $ word: Factor w/ 690 levels "‘똑똑한","‘삶",..: 197 283 297 546 359 374 495 103 169 399 ...
 $ freq: num  91 41 33 31 27 23 16 14 13 13 ...
> pal <- brewer.pal(12, "Paired")
> wordcloud(word.df$word, word.df$freq, scale = c(5, 1),
+ min.freq = 3, random.order = F, 
+ rot.per = .1, colors = pal, family = "malgun")

